{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from pattern.en import pluralize\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_biography = r'(?s)(?:Biography|Biography )\\=\\=(.*?)\\=\\='\n",
    "pattern_cast = r'(?s)Casting and character development\\=\\=(.*?)\\=\\='\n",
    "patter_character_information_coworker = r'(?s)Character information \\=\\=(.*?)Coworker Relations\\=\\='\n",
    "pattern_season = r'(?s)(?:Season [0-9]|Season [0-9] | Season [0-9]\\: )\\=\\=(.*?)(?:\\[\\[Ca|\\=\\=)'\n",
    "pattern_history = r'(?s)(?:Character history|History|Character)\\=\\='\n",
    "pattern_character_information = r'(?s)(?:Background |Overview|Character information|Character Information |Character profile|Character overview)\\=\\=(.*?)\\=\\='\n",
    "pattern_text_only = r'(?s)\\]\\](.*?)\\=\\=(?:Trivia)'\n",
    "pattern_text_only_2 = r'(?s)\\}\\}(.*?)(?:\\[\\[Category)'\n",
    "pattern_text_only_3 = r'(?s)(.*?)(?:\\[\\[Category.*?\\]\\])'\n",
    "\n",
    "pattern1 = r\"(?s)\\[\\[(.*?)(?:\\|.*?)?\\]\\]\"                 # links\n",
    "pattern2 = r\"(?s)\\{\\{(?:cite)(?:.*?)\\}\\}\"               # Cite patterns\n",
    "\n",
    "pattern3 = r\"(?s)\\=\\=\\=(.*?)\\=\\=\\=\"                              # === some text ===\n",
    "pattern4 = r\"(?s)\\{\\{(?:Main)\\|(.*?)\\}\\}\"                            # Main patterns\n",
    "pattern5 = r\"(?s)(?:The Office \\(US TV series|The Office US|The Office \\(UK\\)|The Office \\(US TV series\\)|The Office \\(U.S. TV series\\)|The Office)\" # remove the show name                   # Main patterns\n",
    "pattern6 = r\"(?s)(?:TV)\" # remove the word tv               \n",
    "pattern7 = r\"(?s)\\<.*?\\>(?:.*?)\\<\\/.*?\\>\" # remove ref             \n",
    "pattern8 = r\"(?s)(?:url.*?\\}\\}|\\|)\" # remove urls in https form             \n",
    "pattern9 = r\"(?s)(?:\\[\\[The Office \\(UK\\)\\|.*?)\\]\\]\" # remove more the office           \n",
    "pattern10 = r\"(?s)(?:\\[https.*?)\\]\" # remove more urls           \n",
    "\n",
    "\n",
    "def process_text(name):\n",
    "#     print(f'reading {name}.txt')\n",
    "    res = []                        # initialize the resulting list \n",
    "    \n",
    "    with open(f'data/characters/{name}.txt', encoding=\"utf8\") as file: # 1. open the file\n",
    "        content = file.read()               # 2. read its content\n",
    "        match = ' '.join(re.findall(pattern_biography, content))\n",
    "        \n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_cast, content))\n",
    "        if len(match) < 1:\n",
    "            match += ' '.join(re.findall(patter_character_information_coworker, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_history, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_character_information, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_text_only, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_text_only_2, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_text_only_3, content))\n",
    "                \n",
    "        match += ' '.join(re.findall(pattern_season, content))   \n",
    "    if len(match) < 1:\n",
    "        print(f\"OOOOHHH NOOOO, Coult not find any matches for {name}. -------------------------\")\n",
    "    \n",
    "    updated_match = match\n",
    "    \n",
    "    updated_match = re.sub(pattern1, '', updated_match)\n",
    "    updated_match = re.sub(pattern2, '', updated_match)\n",
    "    updated_match = re.sub(pattern3, '', updated_match) \n",
    "    updated_match = re.sub(pattern4, '', updated_match)\n",
    "    updated_match = re.sub(pattern5, '', updated_match)\n",
    "    updated_match = re.sub(pattern6, '', updated_match)\n",
    "    updated_match = re.sub(pattern7, '', updated_match)\n",
    "    updated_match = re.sub(pattern8, '', updated_match)\n",
    "    updated_match = re.sub(pattern9, '', updated_match)\n",
    "    updated_match = re.sub(pattern10, '', updated_match)\n",
    "\n",
    "    pattern1_match = re.findall(pattern1, updated_match)\n",
    "    pattern3_match = re.findall(pattern3, updated_match) \n",
    "    pattern4_match = re.findall(pattern4, updated_match) \n",
    "\n",
    "    processed_text = updated_match + ' ' + \\\n",
    "                ' '.join(pattern1_match) + ' ' + \\\n",
    "                ' '.join(pattern3_match) + ' ' + \\\n",
    "                ' '.join(pattern4_match) \n",
    "    \n",
    "    # Remove HTML Tags\n",
    "    CLEANR = re.compile('<.*?>') \n",
    "    processed_text = re.sub(CLEANR, ' ', processed_text) \n",
    "    \n",
    "    return processed_text \n",
    "\n",
    "\n",
    "def get_tokens(processed_text, character_name=None, character_names=None):\n",
    "    # 3. Tokenize content\n",
    "    tk = WordPunctTokenizer()\n",
    "    tokens = tk.tokenize(processed_text) \n",
    "    \n",
    "    # 4. Remove non-alpha numeric tokens\n",
    "    tokens = list(filter(lambda t: t.isalpha(), tokens))\n",
    " \n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stop_words.update(['tell', 'say', 'ask', 'asks', 'says', 'later', 'name', 'eventually', \\\n",
    "                       'office', 'episode', 'episodethe', 'become', 'becomes', 'became', 'take', 'show', 'seen', 'see',\\\n",
    "                       'episodes', 'character', 'nbc', 'season', 'wiki', 'deleted', 'portrayed', 'television', 'fictional', 'appears', \\\n",
    "                       'appearance', 'get', 'seasons', 'one', 'series', 'make', 'yes', 'history', 'first', 'titled', 'scene','wikipedia', 'played'])\n",
    "    stop_words.update([\"oh\", \"yeah\", \"ok\", \"dont\", \"hey\", \"okay\", \"know\", \"right\", \"well\", \"fischer\", 'brittany', \"ishibashi\"])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # used to lemantize the words\n",
    "\n",
    "    filtered_tokens = []\n",
    "    for t in tokens: \n",
    "        t_lower = t.lower() # convert text to lowercase\n",
    "        if t_lower in stop_words: continue\n",
    "        if character_name and t_lower == character_name.lower(): continue\n",
    "        t_lower_lemantized = lemmatizer.lemmatize(t_lower) # lemantize the lowercase word\n",
    "        if t_lower_lemantized in stop_words: continue\n",
    "        if character_names and t_lower_lemantized in character_names: continue\n",
    "        filtered_tokens.append(t_lower_lemantized) # add it to the final list of tokens\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dunderpedia_characters.csv')\n",
    "\n",
    "character_names = df.Name\n",
    "lower_pluralized_character_names = []\n",
    "\n",
    "for name in character_names:\n",
    "    n = name.lower()\n",
    "    for n_ in n.split(' '):\n",
    "        lower_pluralized_character_names.append(n_) # separate names and surnames\n",
    "    lower_pluralized_character_names.append(pluralize(n.split(' ')[0]))\n",
    "\n",
    "main_characters = ['Andy Bernard', 'Angela Martin', 'Creed Bratton', 'Darryl Philbin', 'Dwight Schrute', \\\n",
    "                  'Jim Halpert', 'Kelly Kapoor', 'Kevin Malone', 'Meredith Palmer', 'Michael Scott', 'Pam Beesly', \\\n",
    "                  'Phyllis Vance', 'Ryan Howard', 'Stanley Hudson', 'Oscar Martinez', 'Toby Flenderson']\n",
    "\n",
    "main_characters_df = df.loc[df['Name'].isin(main_characters)]\n",
    "main_characters_tokens = {}\n",
    "for index, row in main_characters_df.iterrows():                       # number of files belonging to this race\n",
    "        character_name = row['Name']\n",
    "        processed_text = process_text(character_name) \n",
    "        tokens = get_tokens(processed_text, character_name.split(' ')[0], lower_pluralized_character_names)\n",
    "        main_characters_tokens[character_name] = tokens\n",
    "\n",
    "with open('data/main_characters_tokens.csv', \"w+\") as file:\n",
    "    file.write(json.dumps(main_characters_tokens))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_dict = {}\n",
    "with open('data/communities.csv') as file:\n",
    "    top_five_communities = json.loads(file.read())\n",
    "\n",
    "for community_index, community in top_five_communities:\n",
    "    community_dict[community_index] = []\n",
    "    for document in community:\n",
    "        processed_text = process_text(document)\n",
    "        tokens = get_tokens(processed_text, character_names=lower_pluralized_character_names)\n",
    "        \n",
    "        community_dict[community_index].append(tokens)\n",
    "\n",
    "with open('data/community_dict.csv', \"w+\") as file:\n",
    "    file.write(json.dumps(community_dict)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
