{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import unidecode\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import powerlaw\n",
    "from fa2 import ForceAtlas2\n",
    "import cv2\n",
    "import requests\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gathering data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore The Office, we use several datasets: \n",
    "\n",
    " - We first extract all the characters that played in the Office from season 1 to season 9 (final). This is extracted using **Dunderpedia: The Office Wiki**: https://theoffice.fandom.com/\n",
    " \n",
    " \n",
    " - A description of each character is downloaded from the same source: **Dunderpedia: The Office Wiki**. This data is used to extract the sentiment of each character and will later be compared to the sentiment extracted from the episodes transcripts. We would like to validate that the character description is based on the character developemnt over all seasons. \n",
    " \n",
    " \n",
    " - We then want to analyse episodes ratings The Office and how they change overtime. We use the **IMDb**  to extract: episode title, IMDb rating, total votes ad air date. The source is: https://www.imdb.com/title/tt0386676/episodes/_ajax\n",
    " \n",
    " \n",
    " - From Kaggle we download the guest starts, directors adn writers https://www.kaggle.com/andreal314159/the-office-analysis-for-datacamp/data. We use this data to analyse the impact of guest starts, directors and writers in the ratings. \n",
    " \n",
    " \n",
    " - Next, to further analyse the episodes raitings, we want to get information about the episode itself, such as the characters that played in it, how many lines they had in every episode and so on. This is collected from a **Kaggle dataset: The Office (US) - Complete Dialogue/Transcript**\n",
    "https://www.kaggle.com/nasirkhalid24/the-office-us-complete-dialoguetranscript/version/1?select=The-Office-Lines.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network - Dunderpedia: The Office Wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create our network, we downloaded the Dunderpedia: The Office Wiki pages for all characters in the series and linked them via the hyperlinks connecting pages to each other by examining their contents. To achieve this goal we have used regular expressions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect the hyperlinks from the character pages, we had to extract the list of character names from the The Office API, and saved this information into a csv file: `characters.csv`.  \n",
    "\n",
    "Once the list of The Office characters had been collected, we collect the page information in order to extract the characters race, gender and hyperlinks. \n",
    "\n",
    "To do this we make an API request. We found the baseurl `https://theoffice.fandom.com/api.php?` and title for each of the characters, and we built related API queries for each of them. Then, we made the requests for each query. For each character, we save the resulting page content in a txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the characters data from: https://theoffice.fandom.com/api.php?action=query&format=json&list=categorymembers&cmtitle=Category:Characters&cmlimit=max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title: Category:Characters, the query is: https://theoffice.fandom.com/api.php?action=query&list=categorymembers&cmlimit=max&cmtitle=Category:Characters&format=json\n"
     ]
    }
   ],
   "source": [
    "baseurl = \"https://theoffice.fandom.com/api.php?\" # the baseurl to build the API query, it's common for the three pages\n",
    "action = \"action=query\"\n",
    "\n",
    "# we create a list with the 3 titles from the 3 pages we want to extract the content from\n",
    "title = \"cmtitle=Category:Characters\"\n",
    "\n",
    "# add the parameter list=categorymembers to the query's content\n",
    "content = \"list=categorymembers&cmlimit=max\"\n",
    "dataformat =\"format=json\" # we want the return of the query to be json for future use when we extract the contents\n",
    "\n",
    "# construct the query for the title\n",
    "category_query = f\"{baseurl}{action}&{content}&{title}&{dataformat}\"\n",
    "print(f\"For title: {title.split('=')[1]}, the query is: {category_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get category members and extract the page ids and page titles for all chraracters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib.request.urlopen(category_query) # make the request to the query\n",
    "data_response = response.read().decode('utf-8') # read the response and decode it\n",
    "data_json = json.loads(data_response)['query']['categorymembers'] # convert the response to json format\n",
    "\n",
    "page_ids = []\n",
    "titles = []\n",
    "\n",
    "for category in data_json:\n",
    "    # we keep a list with the page indexes and titles to be used when extracting the content for the category pages\n",
    "    page_ids.append(category['pageid'])\n",
    "    titles.append(category['title'].replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the description for each character. For the future analysis, it is neccessary to extract the characters categories and sub categories, which are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_categories = list(map(lambda title: title.casefold(), titles[:5]))\n",
    "sub_categories = list(map(lambda title: title.casefold().replace('category:', ''), titles[-18:]))\n",
    "characters = titles[5:-18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main categories: ['background_employees', 'clients_of_dunder_mifflin', 'main_characters', 'mentioned_characters', 'voiced_characters']\n",
      "\n",
      "Sub categories: ['actors_of_the_3rd_floor', 'actors_of_threat_level_midnight', 'animals', 'characters_of_dwight_schrute', 'characters_of_michael_scott', 'deceased_characters', 'dunder_mifflin_employees', 'family_members', 'females', 'former_employees', 'it_guys', 'main_characters', 'males', 'the_3rd_floor_characters', 'threat_level_midnight_characters', 'unnamed', 'unseen_characters', 'warehouse_worker']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Main categories: {main_categories}\\n\")\n",
    "print(f\"Sub categories: {sub_categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API pages have been identified, now we extract the information as json format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title: A.J., the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=A.J.&format=json\n",
      "For title: Abby, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Abby&format=json\n",
      "For title: Alan, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Alan&format=json\n",
      "For title: Alan_Brand, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Alan_Brand&format=json\n",
      "For title: Alex, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Alex&format=json\n",
      "For title: Alice, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Alice&format=json\n",
      "For title: Amy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Amy&format=json\n",
      "For title: Andy_Bernard, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Andy_Bernard&format=json\n",
      "For title: Andy_Roddick, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Andy_Roddick&format=json\n",
      "For title: Angela_Martin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Angela_Martin&format=json\n",
      "For title: Angela's_cats, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Angela's_cats&format=json\n",
      "For title: Angelo_Grotti, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Angelo_Grotti&format=json\n",
      "For title: Arnie_Rissman, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Arnie_Rissman&format=json\n",
      "For title: Ashley, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ashley&format=json\n",
      "For title: Asian_Jim, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Asian_Jim&format=json\n",
      "For title: Astrid_Levinson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Astrid_Levinson&format=json\n",
      "For title: Aunt_Carol, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Aunt_Carol&format=json\n",
      "For title: Aunt_Shirley, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Aunt_Shirley&format=json\n",
      "For title: Background_Warehouse_Employees, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Background_Warehouse_Employees&format=json\n",
      "For title: Bandit, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Bandit&format=json\n",
      "For title: Barbara_Allen, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Barbara_Allen&format=json\n",
      "For title: Barbara_Keevis, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Barbara_Keevis&format=json\n",
      "For title: Belsnickel, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Belsnickel&format=json\n",
      "For title: The_Benihana_Trio, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=The_Benihana_Trio&format=json\n",
      "For title: Bert_California, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Bert_California&format=json\n",
      "For title: Bert_Jacobs, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Bert_Jacobs&format=json\n",
      "For title: Betsy_Halpert, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Betsy_Halpert&format=json\n",
      "For title: Bill_Cress, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Bill_Cress&format=json\n",
      "For title: Bill_Hader, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Bill_Hader&format=json\n",
      "For title: Billy_Merchant, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Billy_Merchant&format=json\n",
      "For title: Billy_the_Bartender, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Billy_the_Bartender&format=json\n",
      "For title: Blind_Guy_McSqueezy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Blind_Guy_McSqueezy&format=json\n",
      "For title: Bob_Vance, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Bob_Vance&format=json\n",
      "For title: Brandon, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Brandon&format=json\n",
      "For title: Brenda_Matlowe, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Brenda_Matlowe&format=json\n",
      "For title: Brian_Wittle, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Brian_Wittle&format=json\n",
      "For title: Broccoli_Rob, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Broccoli_Rob&format=json\n",
      "For title: Mr._Brown, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mr._Brown&format=json\n",
      "For title: Bruce, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Bruce&format=json\n",
      "For title: Cameraman, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Cameraman&format=json\n",
      "For title: Captain_Jack, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Captain_Jack&format=json\n",
      "For title: Carla, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Carla&format=json\n",
      "For title: Carla_Fern, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Carla_Fern&format=json\n",
      "For title: Carol_Stills, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Carol_Stills&format=json\n",
      "For title: Casey_Dean, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Casey_Dean&format=json\n",
      "For title: Catherine_Zeta-Scarn, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Catherine_Zeta-Scarn&format=json\n",
      "For title: Cathy_Becker, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Cathy_Becker&format=json\n",
      "For title: Cathy_Simms, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Cathy_Simms&format=json\n",
      "For title: Cecelia_Halpert, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Cecelia_Halpert&format=json\n",
      "For title: Chad_Ligh, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Chad_Ligh&format=json\n",
      "For title: Charles_Miner, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Charles_Miner&format=json\n",
      "For title: Cherokee_Jack, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Cherokee_Jack&format=json\n",
      "For title: Chet_Montgomery, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Chet_Montgomery&format=json\n",
      "For title: Chris_Finch, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Chris_Finch&format=json\n",
      "For title: Chris_Kotter, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Chris_Kotter&format=json\n",
      "For title: Christian, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Christian&format=json\n",
      "For title: Christian_Slater, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Christian_Slater&format=json\n",
      "For title: Clark_(Lactation_Consultant), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Clark_(Lactation_Consultant)&format=json\n",
      "For title: Clark_Green, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Clark_Green&format=json\n",
      "For title: Colin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Colin&format=json\n",
      "For title: Conan_O'Brien, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Conan_O'Brien&format=json\n",
      "For title: Concierge_Marie, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Concierge_Marie&format=json\n",
      "For title: Craig, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Craig&format=json\n",
      "For title: Creed_Bratton, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Creed_Bratton&format=json\n",
      "For title: Cynthia, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Cynthia&format=json\n",
      "For title: Dan_(Karen's_husband), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dan_(Karen's_husband)&format=json\n",
      "For title: Dan_Gore, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dan_Gore&format=json\n",
      "For title: Daniel_Donnelly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Daniel_Donnelly&format=json\n",
      "For title: Danny_Cordray, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Danny_Cordray&format=json\n",
      "For title: Darryl_Philbin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Darryl_Philbin&format=json\n",
      "For title: David_Brent, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=David_Brent&format=json\n",
      "For title: David_Wallace, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=David_Wallace&format=json\n",
      "For title: Dawn_Tinsley, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dawn_Tinsley&format=json\n",
      "For title: Deangelo_Vickers, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Deangelo_Vickers&format=json\n",
      "For title: Debbie_Brown, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Debbie_Brown&format=json\n",
      "For title: Debby_Donnelly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Debby_Donnelly&format=json\n",
      "For title: Deborah_Shoshlefski, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Deborah_Shoshlefski&format=json\n",
      "For title: Dennis, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dennis&format=json\n",
      "For title: Devon_White, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Devon_White&format=json\n",
      "For title: Diane_Kelly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Diane_Kelly&format=json\n",
      "For title: Donna_(UK), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Donna_(UK)&format=json\n",
      "For title: Donna_Newton, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Donna_Newton&format=json\n",
      "For title: Drake_Howard, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Drake_Howard&format=json\n",
      "For title: Dunder_Mifflin_family_members_and_loved_ones, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dunder_Mifflin_family_members_and_loved_ones&format=json\n",
      "For title: Dwight_Schrute, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dwight_Schrute&format=json\n",
      "For title: Ed_Truck, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ed_Truck&format=json\n",
      "For title: Edward_R._Meow, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Edward_R._Meow&format=json\n",
      "For title: Elbert_Lapin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Elbert_Lapin&format=json\n",
      "For title: Eldred, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Eldred&format=json\n",
      "For title: Elizabeth, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Elizabeth&format=json\n",
      "For title: Ellen_Bernard, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ellen_Bernard&format=json\n",
      "For title: Eric, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Eric&format=json\n",
      "For title: Eric_Ward, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Eric_Ward&format=json\n",
      "For title: Erik, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Erik&format=json\n",
      "For title: Erin_Hannon, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Erin_Hannon&format=json\n",
      "For title: Erin's_Father, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Erin's_Father&format=json\n",
      "For title: Erin's_Mother, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Erin's_Mother&format=json\n",
      "For title: Esther_Bruegger, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Esther_Bruegger&format=json\n",
      "For title: Evan, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Evan&format=json\n",
      "For title: Fannie_Schrute, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Fannie_Schrute&format=json\n",
      "For title: Fern_Widgale, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Fern_Widgale&format=json\n",
      "For title: Finger_Lakes_Guy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Finger_Lakes_Guy&format=json\n",
      "For title: Frank, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Frank&format=json\n",
      "For title: Fred_Henry, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Fred_Henry&format=json\n",
      "For title: Gabe_Lewis, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gabe_Lewis&format=json\n",
      "For title: Garbage, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Garbage&format=json\n",
      "For title: Gareth_Keenan, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gareth_Keenan&format=json\n",
      "For title: Gerald_Halpert, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gerald_Halpert&format=json\n",
      "For title: Gideon, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gideon&format=json\n",
      "For title: Gil, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gil&format=json\n",
      "For title: Gino, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gino&format=json\n",
      "For title: Glenn_(Florida), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Glenn_(Florida)&format=json\n",
      "For title: Glenn_(Warehouse_Worker), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Glenn_(Warehouse_Worker)&format=json\n",
      "For title: Glove_Girl, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Glove_Girl&format=json\n",
      "For title: Goldenface, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Goldenface&format=json\n",
      "For title: Gordon, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gordon&format=json\n",
      "For title: Grace, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Grace&format=json\n",
      "For title: Gwyneth_Philbin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Gwyneth_Philbin&format=json\n",
      "For title: Hank_Doyle, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Hank_Doyle&format=json\n",
      "For title: Hannah_Smoterich-Barr, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Hannah_Smoterich-Barr&format=json\n",
      "For title: Helene_Beesly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Helene_Beesly&format=json\n",
      "For title: Hidetoshi_Hasagawa, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Hidetoshi_Hasagawa&format=json\n",
      "For title: Holly_Flax, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Holly_Flax&format=json\n",
      "For title: Honk, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Honk&format=json\n",
      "For title: Hunter_Raymond, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Hunter_Raymond&format=json\n",
      "For title: Irene, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Irene&format=json\n",
      "For title: Isabel_Poreba, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Isabel_Poreba&format=json\n",
      "For title: Ivan_Shotsky, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ivan_Shotsky&format=json\n",
      "For title: Jack, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jack&format=json\n",
      "For title: Jada_Philbin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jada_Philbin&format=json\n",
      "For title: Jake_Palmer, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jake_Palmer&format=json\n",
      "For title: James_P._Albini, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=James_P._Albini&format=json\n",
      "For title: Jamie, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jamie&format=json\n",
      "For title: Jan_Levinson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jan_Levinson&format=json\n",
      "For title: Jasmine_Windsong, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jasmine_Windsong&format=json\n",
      "For title: Jeb_Schrute, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jeb_Schrute&format=json\n",
      "For title: Jerry_DiCanio, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jerry_DiCanio&format=json\n",
      "For title: Jessica_(Andy's_Girlfriend), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jessica_(Andy's_Girlfriend)&format=json\n",
      "For title: Jessica_(Vance_Refrigeration), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jessica_(Vance_Refrigeration)&format=json\n",
      "For title: Jim_Halpert, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jim_Halpert&format=json\n",
      "For title: Jo_Bennett, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jo_Bennett&format=json\n",
      "For title: Jordan_Garfield, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Jordan_Garfield&format=json\n",
      "For title: Julia, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Julia&format=json\n",
      "For title: Julie, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Julie&format=json\n",
      "For title: Julius_Erving, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Julius_Erving&format=json\n",
      "For title: Justine_Philbin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Justine_Philbin&format=json\n",
      "For title: Karen_Filippelli, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Karen_Filippelli&format=json\n",
      "For title: Katy_Moore, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Katy_Moore&format=json\n",
      "For title: Keith_Bishop, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Keith_Bishop&format=json\n",
      "For title: Kelly_Kapoor, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Kelly_Kapoor&format=json\n",
      "For title: Kendall, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Kendall&format=json\n",
      "For title: Kenny_Anderson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Kenny_Anderson&format=json\n",
      "For title: Kevin_Malone, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Kevin_Malone&format=json\n",
      "For title: Kevin's_Sister, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Kevin's_Sister&format=json\n",
      "For title: Kim_Richy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Kim_Richy&format=json\n",
      "For title: Konikotaka, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Konikotaka&format=json\n",
      "For title: Laura_Anderson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Laura_Anderson&format=json\n",
      "For title: Lee, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Lee&format=json\n",
      "For title: Leo, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Leo&format=json\n",
      "For title: Lester_Schneider, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Lester_Schneider&format=json\n",
      "For title: Lily, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Lily&format=json\n",
      "For title: Linda, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Linda&format=json\n",
      "For title: List_of_aliases, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=List_of_aliases&format=json\n",
      "For title: List_of_IT_guys, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=List_of_IT_guys&format=json\n",
      "For title: Lloyd_Gross, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Lloyd_Gross&format=json\n",
      "For title: Lonny_Collins, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Lonny_Collins&format=json\n",
      "For title: Louanne_Kelley, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Louanne_Kelley&format=json\n",
      "For title: Luke_Cooper, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Luke_Cooper&format=json\n",
      "For title: Lynn, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Lynn&format=json\n",
      "For title: Madge_Madsen, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Madge_Madsen&format=json\n",
      "For title: Margaret, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Margaret&format=json\n",
      "For title: Mark, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mark&format=json\n",
      "For title: Mark_Franks, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mark_Franks&format=json\n",
      "For title: Martin_Nash, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Martin_Nash&format=json\n",
      "For title: Marybeth, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Marybeth&format=json\n",
      "For title: Matt, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Matt&format=json\n",
      "For title: Maurie, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Maurie&format=json\n",
      "For title: Megan, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Megan&format=json\n",
      "For title: Melissa_Hudson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Melissa_Hudson&format=json\n",
      "For title: Melissa_Riley, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Melissa_Riley&format=json\n",
      "For title: Melvina_Whitaker, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Melvina_Whitaker&format=json\n",
      "For title: Meredith_Palmer, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Meredith_Palmer&format=json\n",
      "For title: Merv_Bronte, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Merv_Bronte&format=json\n",
      "For title: Michael_(Warehouse_Worker), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Michael_(Warehouse_Worker)&format=json\n",
      "For title: Michael_Klump, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Michael_Klump&format=json\n",
      "For title: Michael_Scarn, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Michael_Scarn&format=json\n",
      "For title: Michael_Scotch, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Michael_Scotch&format=json\n",
      "For title: Michael_Scott, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Michael_Scott&format=json\n",
      "For title: Michael's_mother, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Michael's_mother&format=json\n",
      "For title: Mike_Tibbets, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mike_Tibbets&format=json\n",
      "For title: Mikela_Lasker, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mikela_Lasker&format=json\n",
      "For title: Miserly_Man, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Miserly_Man&format=json\n",
      "For title: Molly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Molly&format=json\n",
      "For title: Mose_Schrute, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mose_Schrute&format=json\n",
      "For title: Mr._Decker, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mr._Decker&format=json\n",
      "For title: Mr._Flax, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mr._Flax&format=json\n",
      "For title: Mr._R._Gould, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mr._R._Gould&format=json\n",
      "For title: Mrs._Flax, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mrs._Flax&format=json\n",
      "For title: Ms._Trudy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ms._Trudy&format=json\n",
      "For title: Mykonos, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mykonos&format=json\n",
      "For title: Nate_Nickerson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Nate_Nickerson&format=json\n",
      "For title: Neil_Godwin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Neil_Godwin&format=json\n",
      "For title: Nellie_Bertram, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Nellie_Bertram&format=json\n",
      "For title: Nick, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Nick&format=json\n",
      "For title: Nick_Figaro, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Nick_Figaro&format=json\n",
      "For title: Mr._O'Malley, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Mr._O'Malley&format=json\n",
      "For title: Oscar_Martinez, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Oscar_Martinez&format=json\n",
      "For title: Pam_(other), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Pam_(other)&format=json\n",
      "For title: Pam_Beesly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Pam_Beesly&format=json\n",
      "For title: Paris, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Paris&format=json\n",
      "For title: Patty_Grossman, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Patty_Grossman&format=json\n",
      "For title: Paul_Faust, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Paul_Faust&format=json\n",
      "For title: Penny_Beesly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Penny_Beesly&format=json\n",
      "For title: Pete_Halpert, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Pete_Halpert&format=json\n",
      "For title: Pete_Miller, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Pete_Miller&format=json\n",
      "For title: Philip, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Philip&format=json\n",
      "For title: Phillip_Halpert, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Phillip_Halpert&format=json\n",
      "For title: Phillip_Schrute, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Phillip_Schrute&format=json\n",
      "For title: Phyllis_Vance, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Phyllis_Vance&format=json\n",
      "For title: Phyllis'_Sister, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Phyllis'_Sister&format=json\n",
      "For title: Ping, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ping&format=json\n",
      "For title: Pizza_Delivery_Kid, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Pizza_Delivery_Kid&format=json\n",
      "For title: Polly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Polly&format=json\n",
      "For title: Josh_Porter, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Josh_Porter&format=json\n",
      "For title: Prison_Mike, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Prison_Mike&format=json\n",
      "For title: Rachael_Martin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Rachael_Martin&format=json\n",
      "For title: Rachel_(Andy's_Girlfriend), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Rachel_(Andy's_Girlfriend)&format=json\n",
      "For title: Rachel_Wallace, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Rachel_Wallace&format=json\n",
      "For title: Randall, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Randall&format=json\n",
      "For title: Ravi, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ravi&format=json\n",
      "For title: Recyclops, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Recyclops&format=json\n",
      "For title: Reed, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Reed&format=json\n",
      "For title: Reggie, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Reggie&format=json\n",
      "For title: Robert_California, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Robert_California&format=json\n",
      "For title: Robert_Dunder, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Robert_Dunder&format=json\n",
      "For title: Robert_Lipton, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Robert_Lipton&format=json\n",
      "For title: Robert_Mifflin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Robert_Mifflin&format=json\n",
      "For title: Rolando, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Rolando&format=json\n",
      "For title: Rolf_Ahl, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Rolf_Ahl&format=json\n",
      "For title: Ronni, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ronni&format=json\n",
      "For title: Rory_Flenderson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Rory_Flenderson&format=json\n",
      "For title: Rose, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Rose&format=json\n",
      "For title: Roy_Anderson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Roy_Anderson&format=json\n",
      "For title: Ryan_Howard, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ryan_Howard&format=json\n",
      "For title: Sadiq, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sadiq&format=json\n",
      "For title: Sam, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sam&format=json\n",
      "For title: Samuel_L._Chang, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Samuel_L._Chang&format=json\n",
      "For title: Sasha_Flenderson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sasha_Flenderson&format=json\n",
      "For title: Sconesy_Cider, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sconesy_Cider&format=json\n",
      "For title: Scranton_Strangler, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Scranton_Strangler&format=json\n",
      "For title: Sensei_Billy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sensei_Billy&format=json\n",
      "For title: Sensei_Ira_Glicksberg, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sensei_Ira_Glicksberg&format=json\n",
      "For title: Seth_(Cece's_Godfather), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Seth_(Cece's_Godfather)&format=json\n",
      "For title: Sophie, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sophie&format=json\n",
      "For title: Sprinkles, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sprinkles&format=json\n",
      "For title: Stacy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Stacy&format=json\n",
      "For title: Stanley_Hudson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Stanley_Hudson&format=json\n",
      "For title: Stephanie, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Stephanie&format=json\n",
      "For title: Steve, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Steve&format=json\n",
      "For title: Steve_(actor_friend), the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Steve_(actor_friend)&format=json\n",
      "For title: Steve_Nash, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Steve_Nash&format=json\n",
      "For title: Sue, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sue&format=json\n",
      "For title: Susan_California, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Susan_California&format=json\n",
      "For title: Sylvia, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Sylvia&format=json\n",
      "For title: Ted_Kassar, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Ted_Kassar&format=json\n",
      "For title: Teddy_Wallace, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Teddy_Wallace&format=json\n",
      "For title: Teresa, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Teresa&format=json\n",
      "For title: Teri_Hudson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Teri_Hudson&format=json\n",
      "For title: The_Gypsy, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=The_Gypsy&format=json\n",
      "For title: The_Killer, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=The_Killer&format=json\n",
      "For title: The_Office_Characters, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=The_Office_Characters&format=json\n",
      "For title: Tiffany, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Tiffany&format=json\n",
      "For title: Tim_Canterbury, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Tim_Canterbury&format=json\n",
      "For title: Toby_Flenderson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Toby_Flenderson&format=json\n",
      "For title: Todd_Packer, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Todd_Packer&format=json\n",
      "For title: Tom_Halpert, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Tom_Halpert&format=json\n",
      "For title: Tom_Peets, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Tom_Peets&format=json\n",
      "For title: Tom_Witochkin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Tom_Witochkin&format=json\n",
      "For title: Tony_Gardner, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Tony_Gardner&format=json\n",
      "For title: Tracy_Fleeb, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Tracy_Fleeb&format=json\n",
      "For title: Trevor_Bortmen, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Trevor_Bortmen&format=json\n",
      "For title: Troy_Underbridge, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Troy_Underbridge&format=json\n",
      "For title: Uncle_Al, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Uncle_Al&format=json\n",
      "For title: Uncle_Lucas, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Uncle_Lucas&format=json\n",
      "For title: Unnamed_Cousin, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Unnamed_Cousin&format=json\n",
      "For title: Unnamed_Paintball_Circumciser, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Unnamed_Paintball_Circumciser&format=json\n",
      "For title: Unnamed_Warehouse_Workers, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Unnamed_Warehouse_Workers&format=json\n",
      "For title: Unnamed_White_Male, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Unnamed_White_Male&format=json\n",
      "For title: Urkel_Grue, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Urkel_Grue&format=json\n",
      "For title: Val_Johnson, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Val_Johnson&format=json\n",
      "For title: Vikram, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Vikram&format=json\n",
      "For title: W.B._Jones, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=W.B._Jones&format=json\n",
      "For title: Wally_Amos, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Wally_Amos&format=json\n",
      "For title: Walter_Bernard_Jr., the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Walter_Bernard_Jr.&format=json\n",
      "For title: Walter_Bernard_Sr., the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Walter_Bernard_Sr.&format=json\n",
      "For title: Wesley_Silver, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Wesley_Silver&format=json\n",
      "For title: William_Beesly, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=William_Beesly&format=json\n",
      "For title: William_M._Buttlicker, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=William_M._Buttlicker&format=json\n"
     ]
    }
   ],
   "source": [
    "content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "\n",
    "queries = []\n",
    "page_ids = page_ids[5:-18]\n",
    "for i, page_id in enumerate(page_ids):\n",
    "    title = characters[i]\n",
    "    query = f\"{baseurl}{action}&{content}&titles={title}&{dataformat}\"\n",
    "    queries.append(query)\n",
    "    print(f\"For title: {title}, the query is: {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we follow the strategy we used to download the pages above, to download each character page. For each character, we save the resulting page content in a txt file.\n",
    "\n",
    "We reuse the baseurl, content, action and dataformat, but use different titles to build the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_roles(content):\n",
    "    roles = {'Main Role': '', 'Secondary Roles': []}\n",
    "    pattern = r'(?:\\[\\[Category\\:)(.*?)\\]\\]'\n",
    "    matches = re.findall(pattern, content)\n",
    "    \n",
    "    for match in matches:\n",
    "        if match.casefold().replace(' ', '_') in main_categories: \n",
    "            roles['Main Role'] = match\n",
    "        elif match.casefold().replace(' ', '_') in sub_categories: \n",
    "            roles['Secondary Roles'].append(match)\n",
    "    return roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = {'Name': [], 'Main Role': [], 'Secondary Roles': []}\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    # make the actual requests using the urllib library for each of the previoysly initialized queries\n",
    "    response = urllib.request.urlopen(query) # make the request to the query\n",
    "    data_response = response.read().decode('utf-8') # read the response and decode it\n",
    "\n",
    "    data_json = json.loads(data_response) # convert the response to json format\n",
    "    name = data_json['query']['pages'][str(page_ids[i])]['title']\n",
    "   \n",
    "    contents['Name'].append(name) # add character name\n",
    "    \n",
    "    # the content that we want to find in the initial page is located at query/pages/'index'/revisions[0]/slots/main/*\n",
    "    content = data_json['query']['pages'][str(page_ids[i])]['revisions'][0]['slots']['main']['*']\n",
    "    \n",
    "    roles = get_character_roles(content) # add character main and secondary roles\n",
    "    if 'Main Role' in roles:\n",
    "        contents['Main Role'].append(roles['Main Role'])\n",
    "    if 'Secondary Roles' in roles:\n",
    "        contents['Secondary Roles'].append(roles['Secondary Roles'])\n",
    "    with open(f'./data/characters/{name}.txt', 'w+') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract characters branches (workplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to extract the different branches of Dunder Mifflin to see in which branch and department every character works at, so we can group them in communities. \n",
    "\n",
    "Using the following page inside the Dunderpedia: https://theoffice.fandom.com/wiki/Branch_(disambiguation), we can see that they are 8 branches. For each branch, we make a request and extract the characters belonging to each branch. Then we add this information to the character dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the queries in the same way as before. Since now we have to make 8 queries, we manually extracted the page ids and titles for each of the pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For title: Dunder_Mifflin_Scranton, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dunder_Mifflin_Scranton&format=json\n",
      "For title: Dunder_Mifflin_Albany, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dunder_Mifflin_Albany&format=json\n",
      "For title: Dunder_Mifflin_Nashua, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dunder_Mifflin_Nashua&format=json\n",
      "For title: Dunder_Mifflin_Corporate_Office, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dunder_Mifflin_Corporate_Office&format=json\n",
      "For title: Dunder_Mifflin_Syracuse, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dunder_Mifflin_Syracuse&format=json\n",
      "For title: Dunder_Mifflin_Utica, the query is: https://theoffice.fandom.com/api.php?action=query&prop=revisions&rvprop=content&rvslots=*&titles=Dunder_Mifflin_Utica&format=json\n"
     ]
    }
   ],
   "source": [
    "content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "\n",
    "branch_queries = []\n",
    "page_ids_branches = [1657, 2011, 2092, 1765, 9709, 2095]\n",
    "titles_branches = ['Dunder_Mifflin_Scranton', 'Dunder_Mifflin_Albany', \n",
    "                   'Dunder_Mifflin_Nashua', 'Dunder_Mifflin_Corporate_Office', \n",
    "                   'Dunder_Mifflin_Syracuse', 'Dunder_Mifflin_Utica']\n",
    "for title in titles_branches:\n",
    "    query = f\"{baseurl}{action}&{content}&titles={title}&{dataformat}\"\n",
    "    branch_queries.append(query)\n",
    "    print(f\"For title: {title}, the query is: {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we make the actual requests and extract the branch and department for the characters listed in the above pages. note that employees can have more than one branch and one department, as this develops during the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chraracter_position(title, content):\n",
    "    pattern = r'\\[\\[(.*?)(?:\\|.*?)?\\]\\](?:.*?)(?:\\- )(.*?)(?:\\;|\\,|\\n|\\<small\\>)'\n",
    "    matches = re.findall(pattern, content)\n",
    "    for match in matches:\n",
    "        character = match[0]\n",
    "        role = match[1]\n",
    "        try:\n",
    "            index = contents['Name'].index(character)\n",
    "            print(index)\n",
    "#             print(contents['Position'][index])\n",
    "            contents['Position'][index].append(role)\n",
    "#             print(contents['Position'][index])\n",
    "            if title.replace('_', ' ') not in contents['Branch'][index]:\n",
    "                contents['Branch'][index].append(title.replace('_', ' '))\n",
    "        except ValueError: # character not present in the list of saved characters\n",
    "#             print(character)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Position'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-54a166eecd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Position'"
     ]
    }
   ],
   "source": [
    "print(contents['Position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casino Night\n",
      "Malcolm\n",
      "Dakota\n",
      "Glenn\n",
      "Mentioned Characters#Tom Peets\n",
      "Mentioned Characters#Marjorie\n",
      "Background Employees\n",
      "Background Employees\n",
      "Mentioned Characters#Jeff\n",
      "Hunter\n",
      "Mentioned Characters#Randall\n",
      "Voiced Characters#Thomas Dean\n",
      "Troy Undercook\n",
      "Voiced Characters#Sherry\n",
      "Harry Jannerone\n",
      "Voiced Characters#Ben Nugent\n"
     ]
    }
   ],
   "source": [
    "contents['Position'] = [[]] * len(contents['Name'])\n",
    "contents['Branch'] = [[]] * len(contents['Name'])\n",
    "\n",
    "for i, query in enumerate(branch_queries):\n",
    "    # make the actual requests using the urllib library for each of the previoysly initialized queries\n",
    "    response = urllib.request.urlopen(query) # make the request to the query\n",
    "    data_response = response.read().decode('utf-8') # read the response and decode it\n",
    "\n",
    "    data_json = json.loads(data_response) # convert the response to json format\n",
    "    name = data_json['query']['pages'][str(page_ids_branches[i])]['title']\n",
    "       \n",
    "    # the content that we want to find in the initial page is located at query/pages/'index'/revisions[0]/slots/main/*\n",
    "    content = data_json['query']['pages'][str(page_ids_branches[i])]['revisions'][0]['slots']['main']['*']\n",
    "    \n",
    "    get_chraracter_position(titles_branches[i], content) # add character position and branch to contents dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents['Role']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract character links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have extracted the contents from each character and we have them in json format. Each page corresponds to a character, which is a node in our network. Next we take the pages we have downloaded for each character. We now find all the hyperlinks in a character's page that link to another node of the network (e.g. another character). For this we use regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(content):\n",
    "    pattern1_matches = re.findall(r'\\[\\[(.*?)(?:\\|.*?)?\\]\\]', content)\n",
    "        \n",
    "    # we need to check if the found matches are characters (i.e. they are present in the extracted df)\n",
    "    pattern1_matches = list(filter(lambda x: x in list(df.Name), pattern1_matches))\n",
    "    return pattern1_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for name in df.Name:\n",
    "    with open(f'./data/characters/{name}.txt') as file:\n",
    "        content = file.read()\n",
    "        # use the get_links to extract the links using a regrex expression\n",
    "        links.append(get_links(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dataframe with the character names, main and secondary roles and links, and we save this data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(5, 'Links', links) # add all genders to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.loc[df['Name'] == 'Dwight Schrute']['Role'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # display the new dataframe with the 3 new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract characters thumbnails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract pictures for each character so that we can use them for the network nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = \"prop=images&rvprop=content&rvslots=*\"\n",
    "\n",
    "img_queries = []\n",
    "\n",
    "for i, page_id in enumerate(page_ids):\n",
    "    title = characters[i]\n",
    "    query = f\"{baseurl}{action}&{content}&titles={title}&{dataformat}\"\n",
    "    img_queries.append(query)\n",
    "    print(f\"For title: {title}, the query is: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = {}\n",
    "errors = []\n",
    "for i, query in enumerate(img_queries):\n",
    "    # make the actual requests using the urllib library for each of the previoysly initialized queries\n",
    "    response = urllib.request.urlopen(query) # make the request to the query\n",
    "    data_response = response.read().decode('utf-8') # read the response and decode it\n",
    "\n",
    "    data_json = json.loads(data_response) # convert the response to json format\n",
    "    name = content = data_json['query']['pages'][str(page_ids[i])]['title']\n",
    "    \n",
    "    try:\n",
    "        # the content that we want to find in the initial page is located at query/pages/'index'/revisions[0]/slots/main/*\n",
    "         \n",
    "        img_title = data_json['query']['pages'][str(page_ids[i])]['images'][0]['title']\n",
    "        img_index = 0\n",
    "        while 'gif' in img_title:\n",
    "            img_title = data_json['query']['pages'][str(page_ids[i])]['images'][img_index+1]['title']\n",
    "\n",
    "        content = 'prop=pageimages'\n",
    "        query_img = f\"{baseurl}{action}&{content}&titles={img_title.replace(' ', '_').replace('&', '%26')}&{dataformat}\"\n",
    "        response = urllib.request.urlopen(query_img) # make the request to the query\n",
    "        img_response = response.read() # read the response and decode it\n",
    "        data_json = json.loads(img_response) # convert the response to json format\n",
    "        pages = data_json['query']['pages']\n",
    "        \n",
    "        page = list(pages.values())[0]\n",
    "        img_url = page['thumbnail']['source']\n",
    "        img_data = requests.get(img_url).content\n",
    "        \n",
    "        extension = img_title.split('.')[-1] if 'gif' not in img_title.split('.')[-1] else 'png'\n",
    "        path =  \"./data/thumbnails/\" + characters[i] + \".\" + extension\n",
    "        images[characters[i]] = characters[i] + \".\" + extension\n",
    "\n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(img_data)\n",
    "    except Exception:\n",
    "        errors.append(characters[i].replace('_', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup the dataset and remove all those characters that do not have a thumbnail, they should not be included in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['Name'].isin(errors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(4, 'ImagePath', images.values()) # add all races to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the to_csv built in pandas function to save our data to a csv file called characters.csv\n",
    "df.to_csv('data/characters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate the The Office network and extract the giant connected component. We use `NetworkX DiGraph` to store the network and we store also the properties of the nodes (i.e. Main Role, Sub role and Image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "# iterate through all the rows in pandas df and add nodes\n",
    "for index, row in df.iterrows():\n",
    "    name = row['Name']\n",
    "    img = row['ImagePath']\n",
    "    main_role = row['Main Role']\n",
    "    sub_roles = row['Secondary Roles']\n",
    "    \n",
    "    # add node \n",
    "    G.add_node(name, img=img, main_role=main_role, sub_roles=sub_roles)\n",
    "    links = row['Links']\n",
    "    for link in links:\n",
    "        if link != name and link in list(df.Name): # do not include links that point to the same character\n",
    "            G.add_edge(name, link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check for isolated nodes, and we remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of isolated nodes in graph: {len(list(nx.isolates(G)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G.remove_nodes_from(list(nx.isolates(G))) # remove isolated nodes in graph\n",
    "print(f\"Number of isolated nodes in graph after removal: {len(list(nx.isolates(G)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the giant connected component by first conecting it to an undirected graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc = max(nx.weakly_connected_components(G), key=len) # find the maximum connected component in G\n",
    "G = G.subgraph(gcc) # the gcc we will operate with\n",
    "G_undirected = G.to_undirected() # convert gcc to undirected graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network visualization and basic stats\n",
    "\n",
    "Visualize the network and calculate stats, we use the GCC to report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What is the number of nodes in the network?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of nodes in network is {len(G.nodes())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What is the number of links?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"The number of links in network is {len(G.edges())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Who is the top connected character in The Office? (Report results for the in-degrees and out-degrees). Comment on your findings. Is this what you would have expected?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the in-degrees and out-degrees we use the `in_degree()` and `out_degree()` functions, respectively. These functions return a collection of tuples representing every node in the network and their respective degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples containing the node (n) and its degree (d) - for the in and out degrees\n",
    "in_degrees = [(n,d) for n,d in G.in_degree()] \n",
    "out_degrees = [(n,d) for n,d in G.out_degree()]\n",
    "\n",
    "# sort the in- and out-degrees from high to low by degree (second element in the tuple - at index 1)\n",
    "in_degrees.sort(reverse=True, key=lambda x: x[1])\n",
    "out_degrees.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "print(f\"The top connected character in The Office for the in-degrees is: {in_degrees[0][0]}, which has a degree of {in_degrees[0][1]}\")\n",
    "print(f\"The top connected character in The Office for the out-degrees is: {out_degrees[0][0]}, which has a degree of {out_degrees[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top connected character in The Office for the in-degrees is Michael Scott, with a 108 in-degree. Michael is the protagonist of The Office, and Regional Manager of the Scranton branch from season 1 to 7. Hence it makes sense that a lot of characters point to him as he is the main character in The Office and has relations to most of the other chraracters. \n",
    "\n",
    "The top connected character in The Office for the out-degrees is Andy Bernard, with a 33 out-degree. In the series, Andy Bernard tries to please everyone, so it makes sense that he is the one pointing to the most characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Who are the top 5 most connected characters (again in terms of in/out-degree)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The top 5 connected character in The Office for the in-degrees are:\")\n",
    "for n, d in in_degrees[:5]: # iterate through the first five nodes and degrees\n",
    "    print(f\"\\t{n}, which has a degree of {d}\")\n",
    "    \n",
    "print(f\"\\nThe top 5 connected character in The Office for the out-degrees are:\")\n",
    "for n, d in out_degrees[:5]:\n",
    "    print(f\"\\t{n}, which has a degree of {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Plot the in- and out-degree distributions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the binned in and out degree distributions, we compute the binning vector of both degrees.\n",
    "To compute the binning vector, we create a list of values spaced by 1 step, where the goal is to have as many bins as unique values in our data such that when the histogram is computed, they range from `min_value` to `max_value`. Hence, when creating the binning-vector, the first element has to be half-step less than the minimum value and the last element half-step more than the maximum value plus one becausethere is one more edge than bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_distribution(degrees):\n",
    "    \"\"\"\n",
    "    Organize the degrees in bins so as to plot the binned degree distribution\n",
    "    \"\"\"\n",
    "    max_degree = max(degrees)\n",
    "    min_degree = min(degrees)\n",
    "\n",
    "    v = np.arange(min_degree-0.5, max_degree+1.5)\n",
    "    bins_count, bins = np.histogram(degrees, v)\n",
    "\n",
    "    # find the middle value between bins and create a new list of bins.\n",
    "    bins_new = []\n",
    "    for index, value in enumerate(bins[0:-1]):\n",
    "        avg = np.mean((value, bins[index+1]))\n",
    "        bins_new.append(avg)\n",
    "    \n",
    "    return bins_new, bins_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degrees = [d for n,d in G.in_degree()] # from the tuple node, degree, get only the degrees\n",
    "out_degrees = [d for n,d in G.out_degree()]\n",
    "\n",
    "# compute the degree distribution in bins for the in- and out-degrees\n",
    "in_bins, in_bins_count = get_degree_distribution(in_degrees)\n",
    "out_bins, out_bins_count= get_degree_distribution(out_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot their distributions and bar and scatter plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "\n",
    "ax[0][0].bar(in_bins, in_bins_count, width=.8)\n",
    "\n",
    "ax[0][0].set_title('Binned in-degree distribution barplot')\n",
    "ax[0][0].set_xlabel(\"degree\", fontsize=10)\n",
    "ax[0][0].set_ylabel(\"count\", fontsize=10)\n",
    "\n",
    "# we use bar and scatter plots of the same distributions for better visualization purposes\n",
    "ax[0][1].scatter(in_bins, in_bins_count)\n",
    "ax[0][1].set_title('Binned in-degree distribution scatter plot')\n",
    "ax[0][1].set_xlabel(\"degree\", fontsize=10)\n",
    "ax[0][1].set_ylabel(\"count\", fontsize=10)\n",
    "\n",
    "ax[1][0].bar(out_bins, out_bins_count, width=.8)\n",
    "ax[1][0].set_title('Binned out-degree distribution barplot')\n",
    "ax[1][0].set_xlabel(\"degree\", fontsize=10)\n",
    "ax[1][0].set_ylabel(\"count\", fontsize=10)\n",
    "\n",
    "ax[1][1].scatter(out_bins, out_bins_count)\n",
    "ax[1][1].set_title('Binned out-degree distribution scatter plot')\n",
    "ax[1][1].set_xlabel(\"degree\", fontsize=10)\n",
    "ax[1][1].set_ylabel(\"count\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **What do you observe?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we can observe is that maximum out-degree is almost 4 times smaller than maximum in-degree, which could have also be seen in the previous question, when we examined the top nodes based on in- and out-degree, Michael and Andy. \n",
    "\n",
    "In the in-degree plots it can be seen that there majority of nodes, nearly 100, have a degree of 0, and the degree ranges until 120. Hence, the distribution shows that a large number of characters point to a small number of characters. While in the out-degree plots, the degree distribution changes slower than the in-degree, and the degrees are more concentrated between 0 and 10, in comparison to the in-degre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Can you explain why the in-degree distribution is different from the out-degree distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the nature of the network. In this network, the link is defined between two characters, $A$ and $B$, if there exists a hyperlink in character's $A$ page that link to $B$'s page. Popular characters will therefore be linked by a lot of other characters, which can accounts for highers in-degrees. Therefore, for less popular characters, will have links pointing to other characters, but very little links pointing to them, which can cause the large amout in-degrees close to 0.\n",
    "\n",
    "The out-degree might be limited by the page length and the fact that every characters includes a low number of hyperlinks in its page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Find the exponent of the degree distribution (by using the `powerlaw` package) for the in- and out-degree distribution. What does it say about our network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_exponent, out_exponent = powerlaw.Fit(in_degrees).alpha, powerlaw.Fit(out_degrees).alpha\n",
    "print(f\"The exponent of the in-degree distribution is {in_exponent:.4f}.\")\n",
    "print(f\"The exponent of the out-degree distribution is {out_exponent:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Compare the degree distribution of the undirected graph to a *random network* with the same number of nodes and probability of connection *p*. Comment your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gu_nodes = len(G_undirected.nodes()) # compute the number of nodes in the undirected graph\n",
    "total_gu_links = len(G_undirected.edges()) # compute the number of edges/links in the undirected graph\n",
    "\n",
    "# Generate a random graph with the same number of nodes and edges (same probability of connection)\n",
    "random_G = nx.gnm_random_graph(n=total_gu_nodes, m=total_gu_links)\n",
    "\n",
    "random_g_bins, random_g_bins_count = get_degree_distribution(list(dict(random_G.degree()).values()))\n",
    "gu_bins, gu_bins_count = get_degree_distribution(list(dict(G_undirected.degree()).values()))\n",
    "\n",
    "fig, (ax_0, ax_1) = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "\n",
    "ax_0.bar(random_g_bins, random_g_bins_count, width=.8)\n",
    "\n",
    "ax_0.set_title(f'Binned degree distribution of a Random network with {total_gu_nodes} nodes and {total_gu_links} edges')\n",
    "ax_0.set_xlabel(\"degree\", fontsize=10)\n",
    "ax_0.set_ylabel(\"count\", fontsize=10)\n",
    "\n",
    "ax_1.bar(gu_bins, gu_bins_count)\n",
    "ax_1.set_title(f'Binned degree distribution of the Undirected graph with {total_gu_nodes} nodes and {total_gu_links} edges')\n",
    "ax_1.set_xlabel(\"degree\", fontsize=10)\n",
    "ax_1.set_ylabel(\"count\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise 1b_: Visualization (see lecture 5 for more hints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create a nice visualization of the total (undirected) network:**\n",
    ">   * Set the nodes icon to be the characters thumbnail\n",
    ">   * Get node positions based on the spring_layout (Force atlas did not work);\n",
    ">   * Whatever else you feel like that would make the visualization nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ForceAtlas2 object with desired parameters\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                          # Behavior alternatives\n",
    "                          outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                          linLogMode=False,  # NOT IMPLEMENTED\n",
    "                          adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                          edgeWeightInfluence=1.0,\n",
    "\n",
    "                          # Performance\n",
    "                          jitterTolerance=1.0,  # Tolerance\n",
    "                          barnesHutOptimize=True,\n",
    "                          barnesHutTheta=1.2,\n",
    "                          multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                          # Tuning\n",
    "                          scalingRatio=1.7,\n",
    "                          strongGravityMode=False,\n",
    "                          gravity=1.0,\n",
    "\n",
    "                          # Log\n",
    "                          verbose=True)\n",
    "\n",
    "# get positions based on the force atlas 2 algorithm\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G_undirected, pos=None, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G_undirected, k = 0.5, iterations=20, scale=1.7) \n",
    "fig = plt.figure( figsize = (100, 100))\n",
    "ax = plt.subplot(111)\n",
    "ax.set_aspect('equal')\n",
    "nx.draw_networkx_edges(G_undirected, pos, ax=ax)\n",
    "\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "\n",
    "trans = ax.transData.transform\n",
    "trans2 = fig.transFigure.inverted().transform\n",
    "\n",
    "piesize = 0.02  # this is the image size\n",
    "p2 = piesize / 2.0\n",
    "for n in G_undirected.nodes:\n",
    "    xx , yy = trans(pos[n]) # figure coordinates\n",
    "    xa , ya = trans2((xx , yy)) # axes coordinates\n",
    "    a  =  plt.axes([xa - p2 , ya - p2 , piesize, piesize])\n",
    "    a.set_aspect('equal')\n",
    "    try :\n",
    "        img = cv2.imread(\"./data/thumbnails/\" + G_undirected.nodes[n]['img'])\n",
    "        a.imshow(img)    \n",
    "    except Exception :\n",
    "        print(n)\n",
    "        continue\n",
    "    a.axis ('off')\n",
    "ax.axis ('off')\n",
    "plt.savefig (\"network.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb: The Office Episode Raitings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we further complement our data with the raitings from each episode of The Offie, for all the 9 seasons using IMDb:https://www.imdb.com/title/tt0386676/episodes/_ajax.\n",
    "\n",
    "\n",
    "IMDb doesn't have official API, so have found the id of The Office in IMDb: tt0386676, next we query the data for each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_url =  \"https://www.imdb.com/title/tt0386676/episodes/_ajax\"\n",
    "number_of_seasons = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the requests library instead of urllib3, and the python `BeautifulSoup` library, as we are making ajax requests.\n",
    "\n",
    "We make one request for each of the 9 seasons. For each request we extract: the season raitings, episode titles, votes, episode air dates and episodes.\n",
    "\n",
    "The result is a data set consisting of 6 attributes, namely: the season of the show, episode number for each rating, title for each episode, IMDb raiting, number of votes, and air date of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raitings_and_votes(soup):\n",
    "    ratings = []\n",
    "    votes = []\n",
    "    episodes = []\n",
    "    \n",
    "    rating_divs = soup.findAll(\"div\", {\"class\": \"ipl-rating-widget\"})\n",
    "    \n",
    "    for index, div in enumerate(rating_divs):\n",
    "        episodes.append(index + 1)\n",
    "\n",
    "        # Find IMDb rating\n",
    "        rating_div_inner = div.findAll(\"div\", {\"ipl-rating-star small\"})\n",
    "        soup_inner_rating = rating_div_inner[0].findAll(\"span\", {\"ipl-rating-star__rating\"})\n",
    "        ratings.append(soup_inner_rating[0].string)\n",
    "\n",
    "        # Find total votes\n",
    "        soup_inner_votes = rating_div_inner[0].findAll(\"span\", {\"ipl-rating-star__total-votes\"})\n",
    "        votes_string = soup_inner_votes[0].string\n",
    "        votes_string = votes_string.replace(',', '')\n",
    "        votes_string = votes_string.replace('(', '')\n",
    "        votes_string = votes_string.replace(')', '')\n",
    "        votes.append(int(votes_string))\n",
    "        \n",
    "    return episodes, ratings, votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_titles(soup):\n",
    "    titles = []\n",
    "    title_divs = soup.findAll(\"strong\")\n",
    "        \n",
    "    for div in title_divs:\n",
    "        titles.append(div.string) \n",
    "        \n",
    "    #Popping the extra title (eg Season 1, Season 2, etcc) at end for each season (not required)\n",
    "    titles.pop()\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airdates(soup):\n",
    "    airdates = []\n",
    "    \n",
    "    airdate_divs = soup.findAll(\"div\", {\"class\": \"airdate\"})\n",
    "    for div in airdate_divs:\n",
    "        airdate_string = div.string\n",
    "        airdate_string = airdate_string.replace('.', '')\n",
    "        airdate_string = airdate_string.strip()\n",
    "        airdates.append(airdate_string)\n",
    "        \n",
    "    return airdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episodes_description(soup):\n",
    "    desciptions = []\n",
    "    \n",
    "    description_divs = soup.findAll(\"div\", {\"class\": \"item_description\"})\n",
    "\n",
    "    for description in description_divs:\n",
    "        description_string = description.string.strip()\n",
    "        desciptions.append(description_string)\n",
    "    \n",
    "    return desciptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.DataFrame([])\n",
    "\n",
    "for season in range(1, number_of_seasons + 1):\n",
    "    print(f'Finding IMDb data for season {season}...')\n",
    "\n",
    "    r = requests.get(url = imdb_url, params = {'season': season})\n",
    "    \n",
    "    # URL response is in HTML format\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    episodes, ratings, votes = get_raitings_and_votes(soup)\n",
    "    titles = get_episode_titles(soup)\n",
    "    airdates = get_airdates(soup)\n",
    "    descriptions = get_episodes_description(soup)\n",
    "\n",
    "    number_of_ep = len(ratings)\n",
    "    seasons = [season] * number_of_ep\n",
    "    \n",
    "    # Preparing data for current season    \n",
    "    data = {'Season': seasons, 'Episode': episodes, 'EpisodeTitle': titles, 'IMDBRating': ratings, 'TotalVotes': votes, 'AirDate': airdates, 'Description': descriptions}\n",
    "    imdb_season_df = pd.DataFrame(data)\n",
    "    imdb_df = imdb_df.append(imdb_season_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we examine the IMDb dataframe and we save it to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further extend this data with the guest starts, directors and writers that we get from Kaggle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle dataset: The Office Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We extract the data from an existing Kaggle dataset and we load it into a dataframe by extracting the following columns: GuestStarts, Director, Writers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.dataset_download_file('nehaprabhavalkar/the-office-dataset','the_office_series.csv', path='./data/kaggle_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "office_df = pd.read_csv('data/kaggle_data/the_office_series.csv', usecols=['Season', 'EpisodeTitle', 'GuestStars', 'Director', 'Writers'])\n",
    "office_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "office_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge this data set with the IMDb data about raitings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = imdb_df.merge(office_df, on=['Season', 'EpisodeTitle'])\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df.to_csv(\"data/imdb_ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Kaggle dataset: The Office (US) - Complete Dialogue/Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the data from an existing Kaggle dataset and we load it into a dataframe to do exploratory data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.dataset_download_file('nasirkhalid24/the-office-us-complete-dialoguetranscript','The-Office-Lines-V4.csv', path='./data/kaggle_data')\n",
    "with zipfile.ZipFile('./data/kaggle_data/The-Office-Lines-V4.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcripts_df = pd.read_csv('data/The-Office-Lines-V4.csv')\n",
    "transcripts_df = transcripts_df.drop('Unnamed: 6', axis=1)\n",
    "\n",
    "transcripts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data downloaded consists of 5 attributes, namely: the season of the show, episode number for each dialogue, the character in the show saying the dialog and the line or dialogue the character says."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is now to perform sentiment analysis of each character in the series and create a wordcloud for each main character in the series. From the data download from the Dunderpedia, we could see that the main characters are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_characters = df.loc[df['Main Role'] == 'Main Characters']['Name']\n",
    "list(main_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to analyse:\n",
    "\n",
    "- Create communities based on the workspace\n",
    "- Create sentment analysis for each character based on their description\n",
    "- Wordclouds for each character based on their descriptions\n",
    "- Explore character relationship based on the lines exchanged between them\n",
    "\n",
    "\n",
    "- Impact on views of each character in relation with the lines spoken\n",
    "- Special guest impact on the views in relation with the average number of views\n",
    "- Rank based on the number of lines spoken in the show\n",
    "- Wordclouds of each episode\n",
    "- Wordclouds for each character based on lines spoken\n",
    "- Sentiment analysis for each episode based on episode description\n",
    "    - Who was the happiest and the saddest character in the series every season?\n",
    "- Character development over the show based on the episode ratings and dialogue\n",
    "- Rank of the highest rated episodes\n",
    "- Top highest voted episodes\n",
    "- Number of guest starts who appear in each season\n",
    "- See how ratings varied from season to season, to see how the popularity of the series changed.\n",
    "- See how each episode director influenced the episode rating.\n",
    "- Compare the rating of episodes directed by the cast in comparison to the other directors.\n",
    "- Top 3 speakers every season and the raiting\n",
    "- Relation between the number of dialogs for the top 3 top speakers and the ratings for each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
