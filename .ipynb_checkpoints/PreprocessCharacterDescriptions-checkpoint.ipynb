{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from pattern.en import pluralize\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_biography = r'(?s)(?:Biography|Biography )\\=\\=(.*?)\\=\\='\n",
    "pattern_cast = r'(?s)Casting and character development\\=\\=(.*?)\\=\\='\n",
    "patter_character_information_coworker = r'(?s)Character information \\=\\=(.*?)Coworker Relations\\=\\='\n",
    "pattern_season = r'(?s)(?:Season [0-9]|Season [0-9] | Season [0-9]\\: )\\=\\=(.*?)(?:\\[\\[Ca|\\=\\=)'\n",
    "pattern_history = r'(?s)(?:Character history|History|Character)\\=\\='\n",
    "pattern_character_information = r'(?s)(?:Background |Overview|Character information|Character Information |Character profile|Character overview)\\=\\=(.*?)\\=\\='\n",
    "pattern_text_only = r'(?s)\\]\\](.*?)\\=\\=(?:Trivia)'\n",
    "pattern_text_only_2 = r'(?s)\\}\\}(.*?)(?:\\[\\[Category)'\n",
    "pattern_text_only_3 = r'(?s)(.*?)(?:\\[\\[Category.*?\\]\\])'\n",
    "\n",
    "pattern1 = r\"(?s)\\[\\[(.*?)(?:\\|.*?)?\\]\\]\"                 # links\n",
    "pattern2 = r\"(?s)\\{\\{(?:cite)(?:.*?)\\}\\}\"               # Cite patterns\n",
    "\n",
    "pattern3 = r\"(?s)\\=\\=\\=(.*?)\\=\\=\\=\"                              # === some text ===\n",
    "pattern4 = r\"(?s)\\{\\{(?:Main)\\|(.*?)\\}\\}\"                            # Main patterns\n",
    "pattern5 = r\"(?s)(?:The Office \\(US TV series|The Office US|The Office \\(UK\\)|The Office \\(US TV series\\)|The Office \\(U.S. TV series\\)|The Office)\" # remove the show name                   # Main patterns\n",
    "pattern6 = r\"(?s)(?:TV)\" # remove the word tv               \n",
    "pattern7 = r\"(?s)\\<.*?\\>(?:.*?)\\<\\/.*?\\>\" # remove ref             \n",
    "pattern8 = r\"(?s)(?:url.*?\\}\\}|\\|)\" # remove urls in https form             \n",
    "pattern9 = r\"(?s)(?:\\[\\[The Office \\(UK\\)\\|.*?)\\]\\]\" # remove more the office           \n",
    "pattern10 = r\"(?s)(?:\\[https.*?)\\]\" # remove more urls           \n",
    "\n",
    "\n",
    "def process_text(name):\n",
    "#     print(f'reading {name}.txt')\n",
    "    res = []                        # initialize the resulting list \n",
    "    \n",
    "    with open(f'data/characters/{name}.txt', encoding=\"utf8\") as file: # 1. open the file\n",
    "        content = file.read()               # 2. read its content\n",
    "        match = ' '.join(re.findall(pattern_biography, content))\n",
    "        \n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_cast, content))\n",
    "        if len(match) < 1:\n",
    "            match += ' '.join(re.findall(patter_character_information_coworker, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_history, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_character_information, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_text_only, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_text_only_2, content))\n",
    "        if len(match) < 1:\n",
    "             match += ' '.join(re.findall(pattern_text_only_3, content))\n",
    "                \n",
    "        match += ' '.join(re.findall(pattern_season, content))   \n",
    "    if len(match) < 1:\n",
    "        print(f\"OOOOHHH NOOOO, Coult not find any matches for {name}. -------------------------\")\n",
    "    \n",
    "    updated_match = match\n",
    "    \n",
    "    updated_match = re.sub(pattern1, '', updated_match)\n",
    "    updated_match = re.sub(pattern2, '', updated_match)\n",
    "    updated_match = re.sub(pattern3, '', updated_match) \n",
    "    updated_match = re.sub(pattern4, '', updated_match)\n",
    "    updated_match = re.sub(pattern5, '', updated_match)\n",
    "    updated_match = re.sub(pattern6, '', updated_match)\n",
    "    updated_match = re.sub(pattern7, '', updated_match)\n",
    "    updated_match = re.sub(pattern8, '', updated_match)\n",
    "    updated_match = re.sub(pattern9, '', updated_match)\n",
    "    updated_match = re.sub(pattern10, '', updated_match)\n",
    "\n",
    "    pattern1_match = re.findall(pattern1, updated_match)\n",
    "    pattern3_match = re.findall(pattern3, updated_match) \n",
    "    pattern4_match = re.findall(pattern4, updated_match) \n",
    "\n",
    "    processed_text = updated_match + ' ' + \\\n",
    "                ' '.join(pattern1_match) + ' ' + \\\n",
    "                ' '.join(pattern3_match) + ' ' + \\\n",
    "                ' '.join(pattern4_match) \n",
    "    \n",
    "    # Remove HTML Tags\n",
    "    CLEANR = re.compile('<.*?>') \n",
    "    processed_text = re.sub(CLEANR, ' ', processed_text) \n",
    "    \n",
    "    return processed_text \n",
    "\n",
    "\n",
    "def get_tokens(processed_text, character_name=None, character_names=None):\n",
    "    # 3. Tokenize content\n",
    "    tk = WordPunctTokenizer()\n",
    "    tokens = tk.tokenize(processed_text) \n",
    "    \n",
    "    # 4. Remove non-alpha numeric tokens\n",
    "    tokens = list(filter(lambda t: t.isalpha(), tokens))\n",
    " \n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stop_words.update(['tell', 'say', 'ask', 'asks', 'says', 'later', 'dunder', 'mifflin', 'name', 'eventually', \\\n",
    "                       'office', 'episode', 'episodethe', 'become', 'becomes', 'became', 'take', 'show', 'seen', 'see',\\\n",
    "                       'episodes', 'character', 'nbc', 'season', 'wiki', 'deleted', 'portrayed', 'television', 'fictional', 'appears', \\\n",
    "                       'appearance', 'get', 'seasons', 'one', 'series', 'make', 'yes', 'history', 'first', 'titled', 'scene','wikipedia', 'played'])\n",
    "    stop_words.update([\"oh\", \"yeah\", \"ok\", \"dont\", \"hey\", \"okay\", \"know\", \"right\", \"well\"])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # used to lemantize the words\n",
    "\n",
    "    filtered_tokens = []\n",
    "    for t in tokens: \n",
    "        t_lower = t.lower() # convert text to lowercase\n",
    "        if t_lower in stop_words: continue\n",
    "        if character_name and t_lower == character_name.lower(): continue\n",
    "        t_lower_lemantized = lemmatizer.lemmatize(t_lower) # lemantize the lowercase word\n",
    "        if t_lower_lemantized in stop_words: continue\n",
    "        if character_names and t_lower in character_names: continue\n",
    "        filtered_tokens.append(t_lower_lemantized) # add it to the final list of tokens\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/characters.csv')\n",
    "\n",
    "character_names = df.Name\n",
    "lower_pluralized_character_names = []\n",
    "\n",
    "for name in character_names:\n",
    "    n = name.lower()\n",
    "    for n_ in n.split(' '):\n",
    "        lower_pluralized_character_names.append(n_) # separate names and surnames\n",
    "    lower_pluralized_character_names.append(pluralize(n.split(' ')[0]))\n",
    "\n",
    "main_characters = ['Andy Bernard', 'Angela Martin', 'Creed Bratton', 'Darryl Philbin', 'Dwight Schrute', \\\n",
    "                  'Jim Halpert', 'Kelly Kapoor', 'Kevin Malone', 'Meredith Palmer', 'Michael Scott', 'Pam Beesly', \\\n",
    "                  'Phyllis Vance', 'Ryan Howard', 'Stanley Hudson', 'Oscar Martinez', 'Toby Flenderson']\n",
    "\n",
    "main_characters_df = df.loc[df['Name'].isin(main_characters)]\n",
    "main_characters_tokens = {}\n",
    "for index, row in main_characters_df.iterrows():                       # number of files belonging to this race\n",
    "        character_name = row['Name']\n",
    "        processed_text = process_text(character_name) \n",
    "        tokens = get_tokens(processed_text, character_name.split(' ')[0], lower_pluralized_character_names)\n",
    "        main_characters_tokens[character_name] = tokens\n",
    "\n",
    "with open('data/main_characters_tokens.csv', \"w+\") as file:\n",
    "    file.write(json.dumps(main_characters_tokens))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6,\n",
       "  ['Alan',\n",
       "   'Kelly Kapoor',\n",
       "   'Ryan Howard',\n",
       "   'Karen Filippelli',\n",
       "   'Alice',\n",
       "   'Pete Miller',\n",
       "   'Andy Bernard',\n",
       "   'Erin Hannon',\n",
       "   'Unnamed Cousin',\n",
       "   'Walter Bernard Jr.',\n",
       "   'Ellen Bernard',\n",
       "   'Robert California',\n",
       "   'Deangelo Vickers',\n",
       "   'Nellie Bertram',\n",
       "   'Gabe Lewis',\n",
       "   'Darryl Philbin',\n",
       "   'Jo Bennett',\n",
       "   'Jamie',\n",
       "   \"Jessica (Andy's Girlfriend)\",\n",
       "   'Carla Fern',\n",
       "   'Ashley',\n",
       "   'Belsnickel',\n",
       "   'Nate Nickerson',\n",
       "   'Bert California',\n",
       "   'Brandon',\n",
       "   'Val Johnson',\n",
       "   'Broccoli Rob',\n",
       "   'Carla',\n",
       "   'Casey Dean',\n",
       "   'Clark Green',\n",
       "   'Colin',\n",
       "   'Julius Erving',\n",
       "   \"Dan (Karen's husband)\",\n",
       "   'Jada Philbin',\n",
       "   'Gwyneth Philbin',\n",
       "   'Drake Howard',\n",
       "   'Reed',\n",
       "   \"Erin's Mother\",\n",
       "   \"Erin's Father\",\n",
       "   'Irene',\n",
       "   'Frank',\n",
       "   'Glenn (Florida)',\n",
       "   'Glenn (Warehouse Worker)',\n",
       "   'Philip',\n",
       "   'Hidetoshi Hasagawa',\n",
       "   'Justine Philbin',\n",
       "   'Jordan Garfield',\n",
       "   'Ravi',\n",
       "   'Lonny Collins',\n",
       "   'Matt',\n",
       "   'Merv Bronte',\n",
       "   'Pam (other)',\n",
       "   'Reggie',\n",
       "   'Susan California',\n",
       "   'Robert Dunder',\n",
       "   'The Gypsy',\n",
       "   'The Killer',\n",
       "   'Tiffany']],\n",
       " [0,\n",
       "  ['A.J.',\n",
       "   'Holly Flax',\n",
       "   'Michael Scott',\n",
       "   'Amy',\n",
       "   'Toby Flenderson',\n",
       "   'Todd Packer',\n",
       "   'Meredith Palmer',\n",
       "   'Arnie Rissman',\n",
       "   'Evan',\n",
       "   'Barbara Allen',\n",
       "   'Barbara Keevis',\n",
       "   'Bert Jacobs',\n",
       "   'Creed Bratton',\n",
       "   'Billy Merchant',\n",
       "   'Mr. Brown',\n",
       "   'Captain Jack',\n",
       "   'Chad Ligh',\n",
       "   'Chet Montgomery',\n",
       "   'Chris Finch',\n",
       "   'Chris Kotter',\n",
       "   \"Conan O'Brien\",\n",
       "   'Devon White',\n",
       "   'Hannah Smoterich-Barr',\n",
       "   'Danny Cordray',\n",
       "   'Louanne Kelley',\n",
       "   'Isabel Poreba',\n",
       "   'Ed Truck',\n",
       "   'Edward R. Meow',\n",
       "   'Eric Ward',\n",
       "   'Erik',\n",
       "   'Finger Lakes Guy',\n",
       "   'Glove Girl',\n",
       "   'Martin Nash',\n",
       "   'Mr. Flax',\n",
       "   'Mrs. Flax',\n",
       "   'Scranton Strangler',\n",
       "   'Jack',\n",
       "   'Jake Palmer',\n",
       "   'Julie',\n",
       "   'Kendall',\n",
       "   'Lloyd Gross',\n",
       "   'Madge Madsen',\n",
       "   'Mark Franks',\n",
       "   'Tony Gardner',\n",
       "   'Marybeth',\n",
       "   'Michael (Warehouse Worker)',\n",
       "   'Michael Klump',\n",
       "   'Ping',\n",
       "   'Prison Mike',\n",
       "   'Mikela Lasker',\n",
       "   'Ms. Trudy',\n",
       "   'Mykonos',\n",
       "   'Nick Figaro',\n",
       "   'Paul Faust',\n",
       "   'Ronni',\n",
       "   'Rory Flenderson',\n",
       "   'Sasha Flenderson']],\n",
       " [1,\n",
       "  ['Pam Beesly',\n",
       "   'Abby',\n",
       "   'Stacy',\n",
       "   'Jim Halpert',\n",
       "   'Alex',\n",
       "   'Cathy Simms',\n",
       "   'Nick',\n",
       "   'Kevin Malone',\n",
       "   'Asian Jim',\n",
       "   'Cecelia Halpert',\n",
       "   'Betsy Halpert',\n",
       "   'Gerald Halpert',\n",
       "   'William Beesly',\n",
       "   'Helene Beesly',\n",
       "   'Pete Halpert',\n",
       "   'Tom Halpert',\n",
       "   'Phillip Halpert',\n",
       "   'Blind Guy McSqueezy',\n",
       "   'Brenda Matlowe',\n",
       "   'Sylvia',\n",
       "   'Donna Newton',\n",
       "   'Dunder Mifflin family members and loved ones',\n",
       "   'Luke Cooper',\n",
       "   'Gil',\n",
       "   'Roy Anderson',\n",
       "   'Fred Henry',\n",
       "   'Penny Beesly',\n",
       "   'Jerry DiCanio',\n",
       "   'Katy Moore',\n",
       "   'Kenny Anderson',\n",
       "   'Lynn',\n",
       "   \"Kevin's Sister\",\n",
       "   'Laura Anderson',\n",
       "   'Lily',\n",
       "   'Sophie',\n",
       "   'Sam',\n",
       "   'Mike Tibbets',\n",
       "   'Polly',\n",
       "   'Rolando',\n",
       "   'Sadiq',\n",
       "   'Sconesy Cider',\n",
       "   \"Seth (Cece's Godfather)\",\n",
       "   'Steve (actor friend)',\n",
       "   'Vikram',\n",
       "   'W.B. Jones']],\n",
       " [5,\n",
       "  ['Oscar Martinez',\n",
       "   'Angela Martin',\n",
       "   'Dwight Schrute',\n",
       "   'Garbage',\n",
       "   'Robert Lipton',\n",
       "   'Phillip Schrute',\n",
       "   'Rachael Martin',\n",
       "   \"Angela's cats\",\n",
       "   'Sprinkles',\n",
       "   'Bandit',\n",
       "   'Angelo Grotti',\n",
       "   'Aunt Shirley',\n",
       "   'Honk',\n",
       "   'Mose Schrute',\n",
       "   'Cameraman',\n",
       "   'James P. Albini',\n",
       "   'Rachel Wallace',\n",
       "   'Concierge Marie',\n",
       "   'Daniel Donnelly',\n",
       "   'Debby Donnelly',\n",
       "   'Teddy Wallace',\n",
       "   'Esther Bruegger',\n",
       "   'Jeb Schrute',\n",
       "   'Fannie Schrute',\n",
       "   'Hank Doyle',\n",
       "   'Ivan Shotsky',\n",
       "   'Konikotaka',\n",
       "   'Linda',\n",
       "   'Mark',\n",
       "   'Melvina Whitaker',\n",
       "   \"Mr. O'Malley\",\n",
       "   'Paris',\n",
       "   'Pizza Delivery Kid',\n",
       "   'Recyclops',\n",
       "   'Wesley Silver',\n",
       "   'Tracy Fleeb',\n",
       "   'Trevor Bortmen',\n",
       "   'Troy Underbridge',\n",
       "   'William M. Buttlicker']],\n",
       " [2,\n",
       "  ['Stanley Hudson',\n",
       "   'Bob Vance',\n",
       "   'Phyllis Vance',\n",
       "   'Cynthia',\n",
       "   'Megan',\n",
       "   'Eric',\n",
       "   'Maurie',\n",
       "   'Dennis',\n",
       "   'Elbert Lapin',\n",
       "   \"Phyllis' Sister\",\n",
       "   'Elizabeth',\n",
       "   'Gino',\n",
       "   'Leo',\n",
       "   'Gordon',\n",
       "   'Jessica (Vance Refrigeration)',\n",
       "   'Julia',\n",
       "   'Melissa Hudson',\n",
       "   'Teri Hudson',\n",
       "   'Rose',\n",
       "   'Sue',\n",
       "   'Uncle Al']]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/top_five_communities.csv', 'r') as file:\n",
    "    top_five_communities = json.loads(file.read())\n",
    "    \n",
    "top_five_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5640c5f0102c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommunity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcommunity_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcommunity_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7bf3d348ee4d>\u001b[0m in \u001b[0;36mget_tokens\u001b[0;34m(processed_text, character_name, character_names)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mt_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert text to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_lower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcharacter_name\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_lower\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcharacter_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mt_lower_lemantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_lower\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lemantize the lowercase word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_lower_lemantized\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/devarea/socialGraphs/project_office/vevn/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "community_dict = {}\n",
    "with open('data/top_five_communities.csv') as file:\n",
    "    top_five_communities = json.loads(file.read())\n",
    "\n",
    "for community_index, community in top_five_communities:\n",
    "    community_dict[community_index] = []\n",
    "    for document in community:\n",
    "        processed_text = process_text(document)\n",
    "        tokens = get_tokens(processed_text, df.Name)\n",
    "        \n",
    "        community_dict[community_index].append(tokens)\n",
    "\n",
    "with open('data/community_dict.csv', \"w+\") as file:\n",
    "    file.write(json.dumps(community_dict)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
